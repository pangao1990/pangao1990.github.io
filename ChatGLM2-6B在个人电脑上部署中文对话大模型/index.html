<!DOCTYPE html><html lang="zh-CN" data-default-color-scheme="auto"><head><meta charset="UTF-8"><link rel="apple-touch-icon" sizes="76x76" href="https://pangao1990.gitee.io/images/favicon.jpg"><link rel="icon" href="https://pangao1990.gitee.io/images/favicon.jpg"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=5,shrink-to-fit=no"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="theme-color" content="#2f4154"><meta name="author" content="潘高"><meta name="keywords" content="Python,Vue,JS,CSS,H5,HTML,Android,iOS,Java,Swift,Perl,微信小程序,开发,博客,潘高"><meta name="description" content="简介ChatGLM2-6B 是清华大学开源的一款支持中英双语的对话语言模型。经过了 1.4T 中英标识符的预训练与人类偏好对齐训练，具有 62 亿参数的 ChatGLM2-6B 已经能生成相当符合人类偏好的回答。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。 使用方式硬件需求   量化等级 最低 GPU(对话) 最低 GPU(微调)    F"><meta property="og:type" content="article"><meta property="og:title" content="ChatGLM2-6B在个人电脑上部署中文对话大模型"><meta property="og:url" content="https://blog.pangao.vip/ChatGLM2-6B%E5%9C%A8%E4%B8%AA%E4%BA%BA%E7%94%B5%E8%84%91%E4%B8%8A%E9%83%A8%E7%BD%B2%E4%B8%AD%E6%96%87%E5%AF%B9%E8%AF%9D%E5%A4%A7%E6%A8%A1%E5%9E%8B/"><meta property="og:site_name" content="潘高的小站"><meta property="og:description" content="简介ChatGLM2-6B 是清华大学开源的一款支持中英双语的对话语言模型。经过了 1.4T 中英标识符的预训练与人类偏好对齐训练，具有 62 亿参数的 ChatGLM2-6B 已经能生成相当符合人类偏好的回答。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。 使用方式硬件需求   量化等级 最低 GPU(对话) 最低 GPU(微调)    F"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pangao1990.gitee.io/pic/index/ChatGLM2-6B%E5%9C%A8%E4%B8%AA%E4%BA%BA%E7%94%B5%E8%84%91%E4%B8%8A%E9%83%A8%E7%BD%B2%E4%B8%AD%E6%96%87%E5%AF%B9%E8%AF%9D%E5%A4%A7%E6%A8%A1%E5%9E%8B.png"><meta property="article:published_time" content="2023-06-30T10:00:00.000Z"><meta property="article:modified_time" content="2023-06-30T08:11:41.762Z"><meta property="article:author" content="潘高"><meta property="article:tag" content="Python"><meta property="article:tag" content="AI"><meta property="article:tag" content="ChatGLM"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:image" content="https://pangao1990.gitee.io/pic/index/ChatGLM2-6B%E5%9C%A8%E4%B8%AA%E4%BA%BA%E7%94%B5%E8%84%91%E4%B8%8A%E9%83%A8%E7%BD%B2%E4%B8%AD%E6%96%87%E5%AF%B9%E8%AF%9D%E5%A4%A7%E6%A8%A1%E5%9E%8B.png"><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5317369003185250" crossorigin="anonymous"></script><meta name="google-site-verification" content="google180fd90b1a12c24d.html"><title>ChatGLM2-6B在个人电脑上部署中文对话大模型 - 潘高的小站</title><link rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.6.1/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/hint.css/2.7.0/hint.min.css"><link rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css"><link rel="stylesheet" href="/css/main.css"><link id="highlight-css" rel="stylesheet" href="/css/highlight.css"><link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css"><script id="fluid-configs">var dntVal,Fluid=window.Fluid||{},CONFIG=(Fluid.ctx=Object.assign({},Fluid.ctx),{hostname:"blog.pangao.vip",root:"/",version:"1.9.4",typing:{enable:!0,typeSpeed:70,cursorChar:"_",loop:!1,scope:[]},anchorjs:{enable:!0,element:"h1,h2,h3,h4,h5,h6",placement:"right",visible:"hover",icon:""},progressbar:{enable:!0,height_px:3,color:"#29d",options:{showSpinner:!1,trickleSpeed:100}},code_language:{enable:!0,default:"TEXT"},copy_btn:!0,image_caption:{enable:!1},image_zoom:{enable:!0,img_url_replace:["",""]},toc:{enable:!0,placement:"right",headingSelector:"h1,h2,h3,h4,h5,h6",collapseDepth:0},lazyload:{enable:!0,loading_img:"/img/loading.gif",onlypost:!1,offset_factor:2},web_analytics:{enable:!0,follow_dnt:!0,baidu:"3cb997f9d46af905b450452a8f181d9e",google:"UA-163962674-1",gtag:null,tencent:{sid:null,cid:null},woyaola:null,cnzz:1278827075,leancloud:{app_id:"w5RAfQXjT6iFfBtGwiTfxQOP-gzGzoHsz",app_key:"h55viQ2cLIrigHjPHDfIS2ly",server_url:"https://w5rafqxj.lc-cn-n1-shared.com",path:"window.location.pathname",ignore_local:!0}},search_path:"/local-search.xml"});CONFIG.web_analytics.follow_dnt&&(dntVal=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,Fluid.ctx.dnt=dntVal&&(dntVal.startsWith("1")||dntVal.startsWith("yes")||dntVal.startsWith("on")))</script><script src="/js/utils.js"></script><script src="/js/color-schema.js"></script><script async>var _hmt;Fluid.ctx.dnt||(_hmt=_hmt||[],function(){var t=document.createElement("script"),e=(t.src="https://hm.baidu.com/hm.js?3cb997f9d46af905b450452a8f181d9e",document.getElementsByTagName("script")[0]);e.parentNode.insertBefore(t,e)}())</script><script async>Fluid.ctx.dnt||Fluid.utils.createScript("https://www.google-analytics.com/analytics.js",function(){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-163962674-1","auto"),ga("send","pageview")})</script><script async src="https://www.google-analytics.com/analytics.js"></script><script async>Fluid.ctx.dnt||Fluid.utils.createScript("//s4.cnzz.com/z_stat.php?id=1278827075&show=pic")</script><meta name="generator" content="Hexo 6.3.0"></head><script async src="https://www.googletagmanager.com/gtag/js?id=AW-715652541"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","AW-715652541")</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-P4XN0EX1GY"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-P4XN0EX1GY")</script><body><header><div class="header-inner" style="height:40vh"><nav id="navbar" class="navbar fixed-top navbar-expand-lg navbar-dark scrolling-navbar"><div class="container"><a class="navbar-brand" href="/"><strong>潘高的小站</strong> </a><button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation"><div class="animated-icon"><span></span><span></span><span></span></div></button><div class="collapse navbar-collapse" id="navbarSupportedContent"><ul class="navbar-nav ml-auto text-center"><li class="nav-item"><a class="nav-link" href="/"><i class="iconfont icon-home-fill"></i> <span>首页</span></a></li><li class="nav-item"><a class="nav-link" href="/categories/"><i class="iconfont icon-category-fill"></i> <span>分类</span></a></li><li class="nav-item"><a class="nav-link" href="/tags/"><i class="iconfont icon-tags-fill"></i> <span>标签</span></a></li><li class="nav-item"><a class="nav-link" href="/archives/"><i class="iconfont icon-shijianzhou"></i> <span>时间轴</span></a></li><li class="nav-item"><a class="nav-link" href="/about/"><i class="iconfont icon-user-fill"></i> <span>关于</span></a></li><li class="nav-item"><a class="nav-link" href="/icp/"><i class="iconfont icon-icp"></i> <span>icp备案</span></a></li><li class="nav-item"><a class="nav-link" href="/links/"><i class="iconfont icon-link-fill"></i> <span>友链</span></a></li><li class="nav-item" id="search-btn"><a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search"><i class="iconfont icon-search"></i></a></li><li class="nav-item" id="color-toggle-btn"><a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle"><i class="iconfont icon-dark" id="color-toggle-icon"></i></a></li></ul></div></div></nav><div id="banner" class="banner" parallax="true" style="background:url(https://pangao1990.gitee.io/pic/banner/JavaScript和Python打造跨平台客户端应用——vue-pywebview-pyinstaller.jpg) no-repeat center center;background-size:cover"><div class="full-bg-img"><div class="mask flex-center" style="background-color:rgba(0,0,0,.2)"><div class="banner-text text-center fade-in-up"><div class="h2"><span id="subtitle" data-typed-text="ChatGLM2-6B在个人电脑上部署中文对话大模型"></span></div><div class="mt-3"><span class="post-meta mr-2"><i class="iconfont icon-author" aria-hidden="true"></i> 潘高 </span><span class="post-meta"><i class="iconfont icon-date-fill" aria-hidden="true"></i> <time datetime="2023-06-30 18:00" pubdate>星期五, 六月 30日 2023, 6:00 晚上</time></span></div><div class="mt-1"><span class="post-meta mr-2"><i class="iconfont icon-chart"></i> 3.7k 字 </span><span class="post-meta mr-2"><i class="iconfont icon-clock-fill"></i> 19 分钟 </span><span id="leancloud-page-views-container" class="post-meta" style="display:none"><i class="iconfont icon-eye" aria-hidden="true"></i> <span id="leancloud-page-views"></span> 次</span></div></div></div></div></div></div></header><main><div class="container-fluid nopadding-x"><div class="row nomargin-x"><div class="side-col d-none d-lg-block col-lg-1"></div><div class="col-lg-9 nopadding-x-md"><div class="container nopadding-x-md" id="board-ctn"><div id="board"><article class="post-content mx-auto"><h1 style="display:none">ChatGLM2-6B在个人电脑上部署中文对话大模型</h1><p class="note note-info">本文最后更新于：2023年6月30日 下午</p><div class="markdown-body"><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM2-6B">ChatGLM2-6B</a> 是清华大学开源的一款支持中英双语的对话语言模型。经过了 1.4T 中英标识符的预训练与人类偏好对齐训练，具有 62 亿参数的 ChatGLM2-6B 已经能生成相当符合人类偏好的回答。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。</p><h2 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h2><h3 id="硬件需求"><a href="#硬件需求" class="headerlink" title="硬件需求"></a>硬件需求</h3><table><thead><tr><th align="center">量化等级</th><th align="center">最低 GPU(对话)</th><th align="center">最低 GPU(微调)</th></tr></thead><tbody><tr><td align="center">FP16（标准）</td><td align="center">13GB</td><td align="center">14GB</td></tr><tr><td align="center">INT8</td><td align="center">8GB</td><td align="center">9GB</td></tr><tr><td align="center">INT4</td><td align="center">6GB</td><td align="center">7GB</td></tr></tbody></table><p>如果没有 GPU 硬件，也可以在 CPU 上进行对话，但是相应速度会更慢。需要大概 32GB 内存。</p><h3 id="安装环境"><a href="#安装环境" class="headerlink" title="安装环境"></a>安装环境</h3><h5 id="下载仓库"><a href="#下载仓库" class="headerlink" title="下载仓库"></a>下载仓库</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Shell">git clone https://github.com/THUDM/ChatGLM2-6B<br><br>cd ChatGLM2-6B<br></code></pre></td></tr></table></figure><h5 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a>创建虚拟环境</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell">python -m venv venv<br></code></pre></td></tr></table></figure><h5 id="激活虚拟环境"><a href="#激活虚拟环境" class="headerlink" title="激活虚拟环境"></a>激活虚拟环境</h5><ul><li>Windows 系统</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell">venv\Script\activate<br></code></pre></td></tr></table></figure><ul><li>macOS&#x2F;Linux 系统</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell">source venv/bin/activate<br></code></pre></td></tr></table></figure><h5 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell">pip install -r requirements.txt -i https://pypi.douban.com/simple<br></code></pre></td></tr></table></figure><h3 id="加载模型"><a href="#加载模型" class="headerlink" title="加载模型"></a>加载模型</h3><p>默认情况下，程序会自动下载模型。奈何模型太大，网络不好的情况下花费时间过长。建议提前下载，从本地加载模型。</p><ul><li><a target="_blank" rel="noopener" href="https://www.aliyundrive.com/s/G7j8vZmZG5C">代码地址</a></li><li><a target="_blank" rel="noopener" href="https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/">模型地址</a></li></ul><p>将下载的 <code>THUDM</code> 文件夹放在 <code>ChatGLM2-6B</code> 文件夹下。文件清单如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs Shell">ChatGLM2-6B<br>│<br>├── THUDM<br>│   ├── chatglm2-6b<br>│   │   ├── MODEL_LICENSE<br>│   │   ├── README.md<br>│   │   ├── config.json<br>│   │   ├── configuration_chatglm.py<br>│   │   ├── modeling_chatglm.py<br>│   │   ├── pytorch_model-00001-of-00007.bin<br>│   │   ├── pytorch_model-00002-of-00007.bin<br>│   │   ├── pytorch_model-00003-of-00007.bin<br>│   │   ├── pytorch_model-00004-of-00007.bin<br>│   │   ├── pytorch_model-00005-of-00007.bin<br>│   │   ├── pytorch_model-00006-of-00007.bin<br>│   │   ├── pytorch_model-00007-of-00007.bin<br>│   │   ├── pytorch_model.bin.index.json<br>│   │   ├── quantization.py<br>│   │   ├── tokenization_chatglm.py<br>│   │   ├── tokenizer.model<br>│   │   └── tokenizer_config.json<br>│   └── chatglm2-6b-int4<br>│       ├── MODEL_LICENSE<br>│       ├── README.md<br>│       ├── config.json<br>│       ├── configuration_chatglm.py<br>│       ├── modeling_chatglm.py<br>│       ├── pytorch_model.bin<br>│       ├── quantization.py<br>│       ├── tokenization_chatglm.py<br>│       ├── tokenizer.model<br>│       └── tokenizer_config.json<br></code></pre></td></tr></table></figure><h3 id="GPU-x2F-CPU-部署"><a href="#GPU-x2F-CPU-部署" class="headerlink" title="GPU&#x2F;CPU 部署"></a>GPU&#x2F;CPU 部署</h3><h5 id="GPU-部署"><a href="#GPU-部署" class="headerlink" title="GPU 部署"></a>GPU 部署</h5><p>默认情况下，程序以基于 GPU 运行。</p><ol><li>查看显卡信息</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell">nvidia-smi<br></code></pre></td></tr></table></figure><p><img src="https://pangao1990.gitee.io/pic/ChatGLM2-6B%E5%9C%A8%E4%B8%AA%E4%BA%BA%E7%94%B5%E8%84%91%E4%B8%8A%E9%83%A8%E7%BD%B2%E4%B8%AD%E6%96%87%E5%AF%B9%E8%AF%9D%E5%A4%A7%E6%A8%A1%E5%9E%8B-1.png" srcset="/img/loading.gif" lazyload alt="image"></p><p>上图表示本机显卡的显存为 8GB，最高支持 CUDA 的版本是 11.2。</p><ol start="2"><li>下载安装 <code>cuda-toolkit</code> 工具</li></ol><p>在 <a target="_blank" rel="noopener" href="https://developer.nvidia.com/cuda-toolkit-archive">这里</a> 选择不高于上述 CUDA 的版本。</p><p><img src="https://pangao1990.gitee.io/pic/ChatGLM2-6B%E5%9C%A8%E4%B8%AA%E4%BA%BA%E7%94%B5%E8%84%91%E4%B8%8A%E9%83%A8%E7%BD%B2%E4%B8%AD%E6%96%87%E5%AF%B9%E8%AF%9D%E5%A4%A7%E6%A8%A1%E5%9E%8B-2.png" srcset="/img/loading.gif" lazyload alt="image"></p><p><img src="https://pangao1990.gitee.io/pic/ChatGLM2-6B%E5%9C%A8%E4%B8%AA%E4%BA%BA%E7%94%B5%E8%84%91%E4%B8%8A%E9%83%A8%E7%BD%B2%E4%B8%AD%E6%96%87%E5%AF%B9%E8%AF%9D%E5%A4%A7%E6%A8%A1%E5%9E%8B-3.png" srcset="/img/loading.gif" lazyload alt="image"></p><p>按提示安装 <code>cuda-toolkit</code> 工具。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Shell">wget https://developer.download.nvidia.com/compute/cuda/11.2.0/local_installers/cuda_11.2.0_460.27.04_linux.run<br>sudo sh cuda_11.2.0_460.27.04_linux.run<br></code></pre></td></tr></table></figure><p>运行以下命令，查看 <code>cuda</code> 是否可用。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell">python -c &quot;import torch; print(torch.cuda.is_available());&quot;<br></code></pre></td></tr></table></figure><p>返回 <code>True</code> 则表示可用。</p><p>在 <code>api.py</code> <code>cli_demo.py</code> <code>web_demo.py</code> <code>web_demo.py</code> 等脚本中，模型默认以 FP16 精度加载，运行模型需要大概 13GB 显存。命令如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">model = AutoModel.from_pretrained(<span class="hljs-string">&quot;THUDM/chatglm2-6b&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>).cuda()<br></code></pre></td></tr></table></figure><p>如果 GPU 显存有限，可以尝试以量化方式加载模型，使用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-comment"># 按需修改，目前只支持 4/8 bit 量化</span><br>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;THUDM/chatglm2-6b&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>).quantize(<span class="hljs-number">4</span>).cuda()<br></code></pre></td></tr></table></figure><p>模型量化会带来一定的性能损失，经过测试，ChatGLM2-6B 在 4-bit 量化下仍然能够进行自然流畅的生成。</p><p>如果内存不足，可以直接加载量化后的模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">model = AutoModel.from_pretrained(<span class="hljs-string">&quot;THUDM/chatglm2-6b-int4&quot;</span>,trust_remote_code=<span class="hljs-literal">True</span>).cuda()<br></code></pre></td></tr></table></figure><h5 id="CPU-部署"><a href="#CPU-部署" class="headerlink" title="CPU 部署"></a>CPU 部署</h5><p>如果没有 GPU 硬件的话，也可以在 CPU 上进行对话，但是对话速度会很慢，需要 32GB 内存（量化模型需要 5GB 内存）。使用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">model = AutoModel.from_pretrained(<span class="hljs-string">&quot;THUDM/chatglm2-6b&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>).<span class="hljs-built_in">float</span>()<br></code></pre></td></tr></table></figure><p>如果内存不足，可以直接加载量化后的模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Python">model = AutoModel.from_pretrained(<span class="hljs-string">&quot;THUDM/chatglm2-6b-int4&quot;</span>,trust_remote_code=<span class="hljs-literal">True</span>).<span class="hljs-built_in">float</span>()<br></code></pre></td></tr></table></figure><p>在 CPU 上运行量化后的模型，还需要安装 gcc 与 openmp。多数 Linux 发行版默认已安装。对于 Windows ，可在安装 <a target="_blank" rel="noopener" href="https://jmeubank.github.io/tdm-gcc/">TDM-GCC</a> 时勾选 openmp。在 MacOS 上请参考 <a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B/blob/main/FAQ.md#q1">这里</a>。</p><h3 id="运行程序"><a href="#运行程序" class="headerlink" title="运行程序"></a>运行程序</h3><h5 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell">python cli_demo.py<br></code></pre></td></tr></table></figure><p>程序会在命令行中进行交互式的对话，在命令行中输入指示并回车即可生成回复，输入 clear 可以清空对话历史，输入 stop 终止程序。如下所示：</p><p><img src="https://pangao1990.gitee.io/pic/ChatGLM2-6B%E5%9C%A8%E4%B8%AA%E4%BA%BA%E7%94%B5%E8%84%91%E4%B8%8A%E9%83%A8%E7%BD%B2%E4%B8%AD%E6%96%87%E5%AF%B9%E8%AF%9D%E5%A4%A7%E6%A8%A1%E5%9E%8B-4.png" srcset="/img/loading.gif" lazyload alt="image"></p><h5 id="网页版-A"><a href="#网页版-A" class="headerlink" title="网页版 A"></a>网页版 A</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell">python web_demo.py<br></code></pre></td></tr></table></figure><p>程序会运行一个 Web Server，并输出地址。在浏览器中打开输出的地址即可使用。最新版 Demo 实现了打字机效果，速度体验大大提升。注意，由于国内 Gradio 的网络访问较为缓慢，启用 <code>demo.queue().launch(share=True, inbrowser=True)</code> 时所有网络会经过 Gradio 服务器转发，导致打字机体验大幅下降，现在默认启动方式已经改为 <code>share=False</code>，如有需要公网访问的需求，可以重新修改为 <code>share=True</code> 启动。如下所示：</p><p><img src="https://pangao1990.gitee.io/pic/ChatGLM2-6B%E5%9C%A8%E4%B8%AA%E4%BA%BA%E7%94%B5%E8%84%91%E4%B8%8A%E9%83%A8%E7%BD%B2%E4%B8%AD%E6%96%87%E5%AF%B9%E8%AF%9D%E5%A4%A7%E6%A8%A1%E5%9E%8B-5.png" srcset="/img/loading.gif" lazyload alt="image"></p><h5 id="网页版-B"><a href="#网页版-B" class="headerlink" title="网页版 B"></a>网页版 B</h5><p>安装 <code>streamlit_chat</code> 模块。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell">pip install streamlit_chat -i https://pypi.douban.com/simple<br></code></pre></td></tr></table></figure><p>运行网页。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell">streamlit run web_demo2.py<br></code></pre></td></tr></table></figure><p>如下所示：</p><p><img src="https://pangao1990.gitee.io/pic/ChatGLM2-6B%E5%9C%A8%E4%B8%AA%E4%BA%BA%E7%94%B5%E8%84%91%E4%B8%8A%E9%83%A8%E7%BD%B2%E4%B8%AD%E6%96%87%E5%AF%B9%E8%AF%9D%E5%A4%A7%E6%A8%A1%E5%9E%8B-6.png" srcset="/img/loading.gif" lazyload alt="image"></p><h5 id="API-部署"><a href="#API-部署" class="headerlink" title="API 部署"></a>API 部署</h5><p>安装 <code>fastapi</code> <code>uvicorn</code> 模块。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell">pip install fastapi uvicorn -i https://pypi.douban.com/simple<br></code></pre></td></tr></table></figure><p>运行 API。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell">python api.py<br></code></pre></td></tr></table></figure><p>默认部署在本地的 8000 端口，通过 POST 方法进行调用。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs Shell">curl -X POST &quot;http://127.0.0.1:8000&quot; \<br>     -H &#x27;Content-Type: application/json&#x27; \<br>     -d &#x27;&#123;&quot;prompt&quot;: &quot;你好&quot;, &quot;history&quot;: []&#125;&#x27;<br></code></pre></td></tr></table></figure><p>得到返回值为</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs Json"><span class="hljs-punctuation">&#123;</span><br>  <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。&quot;</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;history&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;你好&quot;</span><span class="hljs-punctuation">,</span><span class="hljs-string">&quot;你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;status&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-number">200</span><span class="hljs-punctuation">,</span><br>  <span class="hljs-attr">&quot;time&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-string">&quot;2023-06-30 14:51:00&quot;</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><hr><p>更多编程教学请关注公众号：<strong>潘高陪你学编程</strong></p><p><img src="https://pangao1990.gitee.io/pic/%E6%BD%98%E9%AB%98%E9%99%AA%E4%BD%A0%E5%AD%A6%E7%BC%96%E7%A8%8B.jpg" srcset="/img/loading.gif" lazyload alt="image"></p><hr></div><div><hr><div style="margin-top:20px;text-align:center"><div style="margin-bottom:10px">如果这篇文章对你有帮助，或者想给我微小的工作一点点资瓷，请随意打赏。</div><div id="QR" style="display:block"><div id="wechat" style="display:inline-block;margin-right:50px"><img id="wechat_qr" src="https://pangao1990.gitee.io/images/wechatpay.jpg" srcset="/img/loading.gif" lazyload alt="潘高 微信支付" style="width:150px;height:150px"><p style="color:#159718">微信支付</p></div><div id="alipay" style="display:inline-block"><img id="alipay_qr" src="https://pangao1990.gitee.io/images/alipay.png" srcset="/img/loading.gif" lazyload alt="潘高 支付宝" style="width:150px;height:150px"><p style="color:#217cfb">支付宝</p></div></div></div><hr><div class="post-metas my-3"><div class="post-meta mr-3 d-flex align-items-center"><i class="iconfont icon-category"></i> <span class="category-chains"><span class="category-chain"><a href="/categories/ChatGLM/" class="category-chain-item">ChatGLM</a> <span>></span> <a href="/categories/ChatGLM/Python/" class="category-chain-item">Python</a> <span>></span> <a href="/categories/ChatGLM/Python/AI/" class="category-chain-item">AI</a></span></span></div><div class="post-meta"><i class="iconfont icon-tags"></i> <a href="/tags/Python/">#Python</a> <a href="/tags/AI/">#AI</a> <a href="/tags/ChatGLM/">#ChatGLM</a></div></div><div class="license-box my-3"><div class="license-title"><div>ChatGLM2-6B在个人电脑上部署中文对话大模型</div><div>https://blog.pangao.vip/ChatGLM2-6B在个人电脑上部署中文对话大模型/</div></div><div class="license-meta"><div class="license-meta-item"><div>作者</div><div>潘高</div></div><div class="license-meta-item license-meta-date"><div>发布于</div><div>2023年6月30日 晚上</div></div><div class="license-meta-item license-meta-date"><div>更新于</div><div>2023年6月30日 下午</div></div><div class="license-meta-item"><div>许可协议</div><div><a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="BY - 署名"><i class="iconfont icon-by"></i> </span></a><a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="NC - 非商业性使用"><i class="iconfont icon-nc"></i> </span></a><a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/"><span class="hint--top hint--rounded" aria-label="SA - 相同方式共享"><i class="iconfont icon-sa"></i></span></a></div></div></div><div class="license-icon iconfont"></div></div><div class="post-prevnext my-3"><article class="post-prev col-6"></article><article class="post-next col-6"><a href="/JavaScript%E5%92%8CPython%E6%89%93%E9%80%A0%E8%B7%A8%E5%B9%B3%E5%8F%B0%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%BA%94%E7%94%A8%E2%80%94%E2%80%94PPX/" title="JavaScript和Python打造跨平台客户端应用——PPX"><span class="hidden-mobile">JavaScript和Python打造跨平台客户端应用——PPX</span> <span class="visible-mobile">下一篇</span> <i class="iconfont icon-arrowright"></i></a></article></div></div><article id="comments" lazyload><ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-5317369003185250" data-ad-slot="9014150860" data-ad-format="auto" data-full-width-responsive="true"></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script><br><hr><br><div id="waline"></div><script type="text/javascript">Fluid.utils.loadComments("#waline",function(){Fluid.utils.createCssLink("https://cdn.staticfile.org/waline/2.14.1/waline.min.css"),Fluid.utils.createScript("https://cdn.staticfile.org/waline/2.14.1/waline.min.js",function(){var i=Object.assign({serverURL:"https://waline-1-p3250934.deta.app",path:"window.location.pathname",meta:["nick","mail","link"],requiredMeta:["nick"],lang:"zh-CN",emoji:["https://unpkg.com/@waline/emojis@1.1.0/qq","https://unpkg.com/@waline/emojis@1.1.0/weibo"],dark:'html[data-user-color-scheme="dark"]',wordLimit:0,pageSize:10,avatar:"identicon",avatarCDN:"https://seccdn.libravatar.org/avatar/",avatarForce:!1,highlight:!0},{el:"#waline",path:window.location.pathname});Waline.init(i),Fluid.utils.waitElementVisible("#waline .vcontent",()=>{var i="#waline .vcontent img:not(.vemoji)";Fluid.plugins.imageCaption(i),Fluid.plugins.fancyBox(i)})})})</script><noscript>评论功能在允许 JavaScript 运行的环境下效果更佳哦</noscript></article></article></div></div></div><div class="side-col d-none d-lg-block col-lg-2"><aside class="sidebar" style="margin-left:-1rem"><div id="toc"><p class="toc-header"><i class="iconfont icon-list"></i> <span>目录</span></p><div class="toc-body" id="toc-body"></div></div></aside></div></div></div><a id="scroll-top-button" aria-label="TOP" href="#" role="button"><i class="iconfont icon-arrowup" aria-hidden="true"></i></a><div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable modal-lg" role="document"><div class="modal-content"><div class="modal-header text-center"><h4 class="modal-title w-100 font-weight-bold">搜索</h4><button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button></div><div class="modal-body mx-3"><div class="md-form mb-5"><input type="text" id="local-search-input" class="form-control validate"> <label data-error="x" data-success="v" for="local-search-input">关键词</label></div><div class="list-group" id="local-search-result"></div></div></div></div></div><div class="col-lg-7 mx-auto nopadding-x-md"><div class="container custom mx-auto"><link rel="stylesheet" href="//at.alicdn.com/t/font_1765130_cg2uvt55y8.css"></div></div></main><footer><div class="footer-inner"><p id="hitokoto" style="color:#8a2be2">即将出现一段不可描述的话...</p><script>fetch("https://v1.hitokoto.cn").then(t=>t.json()).then(t=>{document.getElementById("hitokoto").innerText=t.hitokoto}).catch(console.error)</script><div><span id="timeDate">正在烧脑计算建站时间...</span> <span id="times"></span><script>var now=new Date;function createtime(){var n=new Date("02/23/2019 00:00:00");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="本站已默默无闻运行&nbsp"+dnum+"&nbsp天",document.getElementById("times").innerHTML=hnum+"&nbsp小时&nbsp"+mnum+"&nbsp分&nbsp"+snum+"&nbsp秒"}setInterval("createtime()",250)</script></div><div class="statistics"><span id="leancloud-site-uv-container" style="display:none">承蒙 <span id="leancloud-site-uv"></span> 位朋友 </span><span id="leancloud-site-pv-container" style="display:none">造访 <span id="leancloud-site-pv"></span> 次</span></div><div class="beian"><span><a href="/icp/blog.pangao.vip.html" target="_blank" rel="nofollow noopener">猿ICP证19901222号</a></span></div><span id="cnzz_stat_icon_1278827075" style="display:none"></span></div></footer><script src="https://cdn.staticfile.org/nprogress/0.2.0/nprogress.min.js"></script><link rel="stylesheet" href="https://cdn.staticfile.org/nprogress/0.2.0/nprogress.min.css"><script>NProgress.configure({showSpinner:!1,trickleSpeed:100}),NProgress.start(),window.addEventListener("load",function(){NProgress.done()})</script><script src="https://cdn.staticfile.org/jquery/3.6.0/jquery.min.js"></script><script src="https://cdn.staticfile.org/twitter-bootstrap/4.6.1/js/bootstrap.min.js"></script><script src="/js/events.js"></script><script src="/js/plugins.js"></script><script src="https://cdn.staticfile.org/typed.js/2.0.12/typed.min.js"></script><script>!function(t){var e=Fluid.plugins.typing,t=t.getElementById("subtitle");t&&e&&e(t.getAttribute("data-typed-text"))}((window,document))</script><script src="/js/img-lazyload.js"></script><script>Fluid.utils.createScript("https://cdn.staticfile.org/tocbot/4.18.2/tocbot.min.js",function(){var t,o=jQuery("#toc");0!==o.length&&window.tocbot&&(t=jQuery("#board-ctn").offset().top,window.tocbot.init(Object.assign({tocSelector:"#toc-body",contentSelector:".markdown-body",linkClass:"tocbot-link",activeLinkClass:"tocbot-active-link",listClass:"tocbot-list",isCollapsedClass:"tocbot-is-collapsed",collapsibleClass:"tocbot-is-collapsible",scrollSmooth:!0,includeTitleTags:!0,headingsOffset:-t},CONFIG.toc)),0<o.find(".toc-list-item").length&&o.css("visibility","visible"),Fluid.events.registerRefreshCallback(function(){var t;"tocbot"in window&&(tocbot.refresh(),0!==(t=jQuery("#toc")).length)&&tocbot&&0<t.find(".toc-list-item").length&&t.css("visibility","visible")}))})</script><script src="https://cdn.staticfile.org/clipboard.js/2.0.11/clipboard.min.js"></script><script>Fluid.plugins.codeWidget()</script><script>Fluid.utils.createScript("https://cdn.staticfile.org/anchor-js/4.3.1/anchor.min.js",function(){window.anchors.options={placement:CONFIG.anchorjs.placement,visible:CONFIG.anchorjs.visible},CONFIG.anchorjs.icon&&(window.anchors.options.icon=CONFIG.anchorjs.icon);var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(window.anchors.options.class="anchorjs-link-left"),window.anchors.add(o.join(", ")),Fluid.events.registerRefreshCallback(function(){if("anchors"in window){anchors.removeAll();var n,o=[];for(n of(CONFIG.anchorjs.element||"h1,h2,h3,h4,h5,h6").split(","))o.push(".markdown-body > "+n.trim());"left"===CONFIG.anchorjs.placement&&(anchors.options.class="anchorjs-link-left"),anchors.add(o.join(", "))}})})</script><script>Fluid.utils.createScript("https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js",function(){Fluid.plugins.fancyBox()})</script><script defer src="/js/leancloud.js"></script><script src="/js/local-search.js"></script><script src="/js/boot.js"></script><noscript><div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳哦</div></noscript></body></html>